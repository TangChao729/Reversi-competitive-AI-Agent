# AI Method 3 - Monte Carlo Tree Search [(Code)](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/agents/t_032/Agent_3.py)

# Table of Contents
- [Monte Carlo Tree Search](#monte-carlo-tree-search)
  - [Motivation](#motivation)
  - [Application](#application)
    - [Node (game state):](#node-game-state)
    - [Selection (differ to choosing an action below):](#selection-differ-to-choosing-an-action-below)
    - [Expansion](#expansion)
    - [Simulation](#simulation)
    - [Backpropagation](#backpropagation)
    - [ChooseAction](#chooseaction)
  - [Solved Challenges](#solved-challenges)
    - [Choose a move within given time](#choose-a-move-within-given-time)
    - [Adding domain knowledge guard for better evaluation](#adding-domain-knowledge-guard-for-better-evaluation)
  - [Trade-offs](#trade-offs)
    - [*Advantages*](#advantages)
    - [*Disadvantages*](#disadvantages)
  - [Future improvements](#future-improvements)

## Monte Carlo Tree Search  

Monte Carlo Tree Search (MCTS) is a search technique that involves both probabilistic and heuristic driven search algorithms. It features a classic tree structure combined with reinforcement learning technic.

### Motivation  
Monte carlo tree search, due to its classic tree structure, is a feasible approach to be implemented in Reversi - a classic state-action game. MCTS has been wildely used in many AI algorithms and it is a well-known AI method for its contribution in AlphaGo, which defeated the best human player in the Go game in 2016.  The intuition of implementing MCTS for reversi game is if an action has a higher probability to win the game over multiple simulations, choosing this action as the next action will have the higher probability to win the game of the current state. Therefore, the overview strategy of the MCTS is to simulate the game from the current game state to the end game state by multiple times and  record each end state's winner and the score. The best next action will be chosen based on the highest the "reward score / visited times".

![](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/reward.GIF)

[Back to top](#table-of-contents)

### Application  

A classic MCTS consist of 4 basic moves: Selection, Expansion, Simulation and Backpropagation. As shown in the diagram below. This diagram is taken from [GeeksforGeeks](https://www.geeksforgeeks.org/ml-monte-carlo-tree-search-mcts/).

![](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/mcts_own.png)

#### Node (game state):
A node is consist of several variables:

1) parent -> the parent node of this node
2) reward  -> current reward point of this node
3) visit -> current total visited times of this node
4) color -> represented by the player ID
5) current -> indicate the current move action
6) childen -> a dictionary of all its children nodes

#### Selection (differ to choosing an action below):

MCTS starting with selecting a unvisited node. The strategy of selecting is:
1) If the node is unvisited before, choose it
2) If the node is visited, check its children nodes, pick anyone that is unvisited
3) If the node is visited, and all its children nodes are visited, choose the child node with highest UCT score (detail refer to sections below) and set the chosen child node as the parent node, starting from the step 1 until finding a unvisited node.

The code implementation is shown below:

```python

def Select(self, node, state):
        # Handle timeout
        if time.time() - self.start_time >= self.time_limit:
            return False

        # If there is no child for the current node, choose this node
        if len(node.child) == 0:
            return node, state
        else:
            # Init
            best_score = float("-inf")
            best_move = None

            for k in node.child.keys():
                # If the current child is not visited, choose this child node
                if node.child[k].visit == 0:
                    best_move = k
                    break
                # If the current child is visited, choose a child node based on the UCT formula
                else:
                    child_visit = node.child[k].visit
                    # UCT score
                    score = (node.child[k].reward / child_visit) + 2 * math.sqrt(2 * math.log(node.visit) / child_visit)
                    if score > best_score:
                        best_score = score
                        best_move = k

            state = self.ExecuteAction(state, best_move, node.color)
            return self.Select(node.child[best_move], state)
```


#### Expansion:

After selecting a unvisited (un-expanded) node, the algorithm expand this node, to expand all its child nodes.
In the reversi game, this is presented with all actions available to a player in a game board, and the corresponding board resulted from this action.

A demonstration of expansion in the reversi game is shown below:

![alt text](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/mcts_expand.png)

The code implementation is shown below:

```python
def Expand(self, node, state):
        # Expand the child node
        for move in self.GetActionList(state, node.color):
            node.child[move] = TreeNode(node, move, 1 - node.color)

```

#### Simulation:

After selecting a unvisited (un-expanded) node, the algorithm also run a simulation based on the this unvisited node. Originally, the simulation is based two player each make a random move until end of the game. 

In the reversi game, this is represented by two player (Agent 0 & Agent 1) each make a move based on current game board setting and rules (such as who is the next player to move). Each player's action is based a separate function involves with hardcode-guided action selection until end of the game. The end of game score (the number differences between white pieces and black pieces) is then passed to backpropagation process.

```python
def Simulation(self, node, state):
        # Change the color first
        if node.color == self.id:
            current_color = 1 - self.id
        else:
            current_color = self.id

        # Simulate until the game end
        while not self.GameEnd(state):

            # Handle the timeout
            if time.time() - self.start_time >= self.time_limit:
                return False

            # Get the current color and choose an action
            current_color = 1 - current_color
            action_list = self.GetActionList(state, current_color)
            action = self.DetermineAction(action_list)

            # If chosen action is 'Pass', continue
            if action == 'Pass':
                continue
            # If chosen action is not 'Pass', execute the current action
            else:
                state = self.ExecuteAction(state, action, current_color)

        # Calculate the reward and record the winner
        winner, score = self.CalculateReward(state)

        return winner, score
```

#### Backpropagation:

After the simulation ends, we now record the result for this node, and all its parent nodes.

In the reversi game, after we have a score including whether black or white is winner, the score differences between their pieces. This data is then being recorded at this node. Based on the current node color, if it is the same color as the winner's color, the "reward" variable will add with the score. Otherwise, the "reward" variable will minus the score. The "visited" variable is always plus 1. We then pass the score information to all its parent node for doing the same recording.

```python
def BackPropagation(self, node, score):
        if time.time() - self.start_time >= self.time_limit:
            return False
        node.visit += 1
        if node.color == self.id:
            node.reward += score
        else:
            node.reward -= score
        if node.parent is not None:
            self.BackPropagation(node.parent, score)
```

#### ChooseAction:

When we need to choose an action, we compare all the available actions based on its corresponding node variables: "reward" and "visited". We choose the one with highest reward ratio with "reward / visited".

In the reversi game, from the current node, we choose the highest reward ratio node based on each node's reward score and visited times.

```python
def FindSolution(self, root):
        best_n = float("-inf")
        best_move = None

        for k in root.child.keys():
            if root.child[k].visit > 0:
                if (root.child[k].reward / root.child[k].visit) > best_n:
                    best_n = root.child[k].reward / root.child[k].visit
                    best_move = k
        return best_move
```


[Back to top](#table-of-contents)

### Solved Challenges

#### Choose a move within given time

In this game rule, each player only has 1 second to choose a move in each round (apart from the 1st round, which has 15s).

A traditional MCTS method may stop the searching at a given depth. However, with only 1 second of thinking time, it is not appropriate to set a required depth to reach.

Therefore, to cope with the given rule, we use a timer to track the time elapsed. As long as we are still within the given 1 second time frame, the MCTS tree keep expanding and simulating. After reach 1 second, we choose the best move accordingly.

NOTE: below code is streamlined to demonstrate the time guard feature.
```python
def MCTS(self, game_state):
        root = TreeNode(None, None, self.id)
        while time.time() - self.start_time < self.time_limit:
            # Select
            select_ans = self.Select(root, deepcopy_board)

            # Expand
            self.Expand(choice, deepcopy_board)

            # Simulation
            simulation_ans = self.Simulation(choice, deepcopy_board)

            # Calculate the reward
            if self.id == 1:
                propagation_score = [different, -different, 0][winner]
            else:
                propagation_score = [-different, different, 0][winner]

            # back propagation
            self.BackPropagation(choice, propagation_score)

        return self.FindSolution(root)
```

#### Adding domain knowledge guard for better evaluation

The original simulation process was set to let players making random moves until game ends. However this approach cannot best reflect the current state's situation. It is highly likely that one player at a winning situation lose due to making a random bad move. Although this single result is minimized by running hundreds of simulations and it is indeed embedded with the very nature of this method (as MCTS's name suggests that monte carlo refers to the gamble city), we could still use some hard-coded action determiner to better simulate a game.

Below is hard-coded board weights, this is used to encourage each player to take "high value" blocks such as corners and edges, and discourage players to take "high risk" blocks such as inner-corners.

```python
self.hard_code = [
    [(0, 0), (7, 0), (0, 7), (7, 7)],
    [(2, 2), (2, 3), (2, 4), (2, 5), (3, 2), (3, 5), (4, 2), (4, 5), (5, 2), (5, 3), (5, 4), (5, 5)],
    [(0, 2), (0, 3), (0, 4), (0, 5), (2, 0), (3, 0), (4, 0), (5, 0), (7, 2), (7, 3), (7, 4), (7, 5), (2, 7),
      (3, 7), (4, 7), (5, 7)],
    [(2, 1), (3, 1), (4, 1), (5, 1), (1, 2), (1, 3), (1, 4), (1, 5), (6, 2), (6, 3), (6, 4), (6, 5), (2, 6),
      (3, 6), (4, 6), (5, 6)],
    [(0, 1), (7, 1), (0, 6), (7, 6), (1, 0), (6, 0), (1, 7), (6, 7),(1, 1), (6, 1), (1, 6), (6, 6)]
]
```

[Back to top](#table-of-contents)


### Trade-offs  
#### *Advantages*  

- Less domain knowledge required to implement MCTS is a reinforcement learning algorithm which can build its own "understanding" during expanding and simulation.
- The MCTS perform well in the middle and end of the game since it can simulate more than 50 games. 
- The MCTS can save any intermediate game state of the reversi, and these game states can be used in the future if needed. Therefore, if there is no the time limit, the MCTS can simulation all possible game states to find the optimal action.
- MCTS utilizes game theory which provides better performance in a this game by considering the opponent's move.
- The UCT function can allow MCTS to expand the node with better performance, which increase the effectiveness and efficiency of the node expansion and simuluation of MCTS. Therefore, the UCT can provide the more reliable decision making of the next action.

#### *Disadvantages*

- Due the time limit of the action selection, it is impossible for MCTS simulate all possible game state. Therefore, the action chosen by the MCTS may not optimal. 
- In the early stage of the game, the MCTS perform unwell due to the tree are deep. Therefore, the MCTS can only simulation few times, which result in provide the poor action.
- Require fine tunning between exploitation and exploration ratio.
- The result of the MCTS is highly based on the simulation result and sometimes MCTS may loss some important position of the reversi game. For example, the corner position is very important poistion for the reversi game. However, the MCTS often loses the game by not picking the corner position. This siuation is shown in the following GIF.

![](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/mcts_weakness.gif)

[Back to top](#table-of-contents)

### Future improvements  
 
- More sophisticated hardcode domain knowledge implementation.
  The current hard-coded board weight map is static and can sometimes mislead the player not to choose a good move. This is due to it cannot reflect back on the current game state's situation. An improvement such as dynamic weighting table, which could change the weight of each block based current board situation could be helpful.

- More sophisticated UCT value calculation. 
  The current UCT score is also too simple, it calculates the reward / visited ratio. An improvement could be adding an extra layer of calculation to differentiate high value blocks (i.e. corners) to other blocks, or penalize bad moves (i.e. inner-corners).

- More sophisticated simulation scoring calculation
  The current simulation scoring is simple, it only calculates the pieces differences. An improvement could be also calculating the corners the winner has taken. In this way, it provides an extra layer of guarantee to ensure this simulation hold true in most cases.

- Dynamic exploitation and exploration ratio
  The c-value can be modified to change dynamically in a game. For example, during starting stage of the game, we encourage MCTS to explore more options by tuning up the c-val, and during end stage of the game, we tune down the c-val to let MCTS to focus on calculating the best winning approach.
  
[Back to top](#table-of-contents)