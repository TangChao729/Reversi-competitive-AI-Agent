During the pre-competition state, our group have tried several methods with different implementations.  

## Our First Agent - [BFS](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/agents/t_032/Agent_1.py)


Our first agent was merely a breadth first search tree with a bit touch of domain knowledge assist, which is a good try on getting us familiar with the game rule and the AI agent. However, due to the limit time of action selection, the pure BFS cannot search all possible end game state. Therefore, the pure BFS performed unwell at the beginning. After that, we added the hard-code of the corner strategy and the stable position strategy described in the [method-1](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/wiki/2.1-Method-1---Breadth-First-Search) to improve the BFS performance. Finally, this BFS agent can achieve the 50th in the Tournaments.

### Demo

![](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/agent_1.gif)

#### Pre-Competition results: Placed - 50th | Win/Lose - 46.7%

![alt txt](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/50_BFS_47percent.png)


## Our Second Agent - [MINIMAX](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/agents/t_032/Agent_2.py)


Upon on some research, we found that minimax is a suitable approach. Our first version minimax was greedy (always consider that best move based on heuristic value) and its heuristic function is simply calculating the places taken on the board, which result in the next action chosen by the minimax is not the global optimal. However, this minimax agent is still perform better than that of the BFS and achieves the 43th in the Tournaments.

### Demo

![](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/agent_2.gif)

#### Pre-Competition results: Placed - 43th | Win/Lose - 49.2%

![alt txt](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/43_minimax_50percent.png)



## Our Third Agent - [MCTS](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/agents/t_032/Agent_3.py)


During implementing minimax approach, we realized that to be able to perform really well, our minimax agent needs a strong heuristic function that can correctly evaluate the current board situation beyond only counting blocks taken and this heuristic function relevant to the high domain knowledge of the reversi game. The team only has the limit domain knowledge about the reversi game. Therefore, it is hard for team to implement a excellent heuristic function.

Therefore, We shift our focus to MCTS which uses its reinforcement learning feature which can build its own "understanding" during expanding and simulation. A vanilla version of MCTS outperformed our minimax agent by a decent margin. After we using the UCT fomular to expand the node, the performance of the MCTS is much stronger than that of minimax and achieves the 24th in the Tournaments.

### Demo

![](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/agent_3.gif)

#### Pre-Competition results: Placed - 24th | Win/Lose - 61.2%

![alt txt](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/24_mcts.png)


## Our Final Agent - [Hybrid MCTS + MINIMAX](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/agents/t_032/Agent_4.py)


A vanilla version of MCTS does not contain any domain knowledge to the reversi game. Under the restriction of 1 second thinking time, it is also quite vulnerable during the starting stage of the game. Hence, we introduce our hybrid agent.

This hybrid agent undertakes the hard-coded domain knowledge (weighted board, stability and mobility analysis), and uses minimax as the main method for the first 10 moves and MCTS as the method for the rest of the game process.

With a bit tuning and testing described in [method-4](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/wiki/2.4-Method-4---Monte-Carlo-Tree-Search-Plus-Minimax), our hybrid agent gained a substantial improvements compared with the pure MCTS and achieves the 14th in the Tournaments.

### Demo

![](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/agent_4.gif)

#### Pre-Competition results: Placed - 14th | Win/Lose - 67%

![alt txt](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/17_minimax+mcts_67percent.png)

## Compare the performance of three agents
The following table show the winning rate of the competition between our agent with 100 games. The agent in the table horizontal is the opponent. 

The random agent is the base line of AI agent, which means that the AI agent at least smarter than random. According to this table, all our agents can easily beat random. Our final approach MCTS + Minimax has the highest average winning rate(0.885) out of all our agents and the winning rate of this agent has more than 50% against all other agents.

| Agent Name                      | Random | Breadth First Search (BFS) | Minimax | Monte Carlo Tree Search (MCTS) | Final approach - MCTS + Minimax | Average |
|---------------------------------|--------|----------------------------|---------|--------------------------------|---------------------------------|---------|
| Random                          | None   | 0.16                       | 0.19    | 0.07                           | 0.02                            | 0.11    |
| Breadth First Search (BFS)      | 0.84   | None                       | 0.35    | 0.18                           | 0.09                            | 0.365   |
| Minimax                         | 0.81   | 0.63                       | None    | 0.15                           | 0.04                            | 0.4075  |
| Monte Carlo Tree Search (MCTS)  | 0.93   | 0.75                       | 0.83    | None                           | 0.28                            | 0.6975  | 
| Final approach - MCTS + Minimax | 0.98   | 0.90                       | 0.95    | 0.71                           | None                            | 0.885   |
