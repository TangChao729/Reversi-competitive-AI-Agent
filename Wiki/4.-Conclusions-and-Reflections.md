## 1. Conclusion 

In conclusion, the team implements the AI agent of reversi game by using three AI-Technical which are the Breadth First Search, Monte Carlo Tree Search with UCT and the Minimax. Although the MCTS perform best out of three agents, the team still seeking a way to improve its performance. Finally, the team determined to use the combination of the MCTS and the minimax as our final agent, which combine the advantage of both MCTS and the Minimax and has the better performance compared with only using the MCTS.

During the evolution of our agents, we conclude that a good agent must consist of 1) good guidance algorithm, 2) associated domain knowledge, 3) efficient code, to perform well in the given competition environment. This requires us to conduct thorough researches regarding reversi and AI algorithms, carefully design the algorithm implementation, put it under tests and tune it frequently, and being creative to utilize the knowledge we gained comprehensively.

Our research and implementation of Reversi AI agents are limited due to restricted time frame. There are many other great AI algorithm we have not tried, and there are many improvements we could have done to our agents. As a result, our agent did not performed the best among 80+ groups (the best result we achieved so far is at 14th). But what we learned from the evolution process of our agents is that multiple AI algorithms backed up with theory supports and associated with deep understanding of game rules are the key elements for a good agent.


## 2. Reflection

As students who intake this course as our first AI course, we have learned a lot AI algorithms during the development process. We started with simple BFS algorithm, to game-theory related algorithm MINIMAX and reinforcement learning algorithm MCTS. During the development, we configure the implementations and reflect on the tests results. In this process, we constantly learn, improve and review the concepts from the first-handed experiences, which leads us to analysis the code we have written, what are the advantages and disadvantages, and how could we improve it. 

Moreover, we have learned a great deal of team cooperation. During the development, we worked cooperatively together. At the beginning we were struggling with a less efficient communication process. But with us getting to know others better, the communication process becomes high efficient. Each of us contributed equally in this assignment in terms of doing one's own research and code implementation. Whenever one group member needs help, the others come to rescue. We all have kept an open mind during discussion to encourage others being creative.

Hence, we believe we all have learned a lot in this assignment, both from AI algorithm's perspective and team-work perspective.

## 3. Learning outcomes

1) Reversi game rules and strategies
2) Tree-like structure algorithm implementation in state-action based games
3) Game theory implementation in state-action based games
4) MINIMAX algorithms including detailed implementation, advantages and disadvantages
5) MCTS algorithms including detailed implementation, advantages and disadvantages
6) A try-out on combination of multiple algorithms