# AI Method 2 - Minimax [(Code)](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/agents/t_032/Agent_2.py)

# Table of Contents

- [Minimax](#minimax)
  * [Motivation](#motivation)
  * [Application](#application)
  * [Solved challenges](#solved-challenges)
  * [Trade-offs](#trade-offs)     
    - [Advantages](#advantages)
    - [Disadvantages](#disadvantages)
  * [Future improvements](#future-improvements)

## Minimax  

### Motivation  

To create an agent that can get as many points as possible from the opponent agent, we choose to use the Minimax algorithm in Method 2. Minimax is a game theory algorithm commonly used in zero-sum games and suitable for perfect information games. Therefore, the minimax strategy is suitable to use in the reversi game. The main idea of Minimax is to minimize the loss under the assumption that the opponent agent's decision is perfect, which means we can improve our score when the opponent agent's decision is not the best.

[Back to top](#table-of-contents)

### Application  

The implementation of Minimax mainly needs to solve the following problems:

- How to create the evaluation function of the current board

The establishment of evaluation function is the core factor to measure the accuracy of a Minimax model. In this question, we need to evaluate the value of the current board to our agent and the opponent agent. After getting the value of the simulated board, only can we use Minimax to calculate which step is most suitable for our current situation.

The evaluation function our group uses here is to calculate how many pieces of our side and the opponent side exist on the current board. This method is very easy to implement.

```python
def evaluation(self, game_state, agent_id):
    score = 0
    for i in range(self.BOARD_SIZE):
        for j in range(self.BOARD_SIZE):
            if game_state.board[i][j] == self.color[agent_id]:
                score += 1
            elif game_state.board[i][j] == self.color[1 - agent_id]:
                score -= 1
    return score
```

- How to implement Minimax model

![](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/minimax_model.png)

The core of minimax is to deduce the actions of our side and the other side. When deducing to our round, we will use the evaluation function mentioned above to evaluate all possible actions, and select our most benefit action, that is, select the maximum value of all evaluations. When deducing to the other side's round, because we assume that the other side is a perfect agent, the other side will choose the action of less benefit, that is, the minimum value of all evaluations.

```python
def minimax(self, state, agent_id, alpha, beta, depth):

    if depth == self.depth or self.is_game_end(state) or time.time() - self.start_time >= self.time_limit:
        return None, - self.calculate_weight(state, agent_id)

    action_list = self.get_action_list(state, agent_id)
    best_action = None

    # If the self.id equal to the current agent id - max level
    if agent_id == self.id:
        max_value = float('-inf')
        for action in action_list:

            new_state = self.execute_action(state, action, agent_id)
            _act, value = self.minimax(new_state, 1 - agent_id, alpha, beta, depth + 1)

            if value > max_value:
                max_value = value
                best_action = action

            alpha = max(alpha, value)

            if beta <= alpha:
                return best_action, max_value

        return best_action, max_value

    # If self.id not equal to the current agent id - min level
    elif agent_id != self.id:
        min_value = float('inf')
        for action in action_list:
            new_state = self.execute_action(state, action, agent_id)
            _act, value = self.minimax(new_state, 1 - agent_id, alpha, beta, depth + 1)

            if value < min_value:
                min_value = value
                best_action = action

            beta = min(beta, value)

            if beta <= alpha:
                return best_action, min_value

        return best_action, min_value
```

An example of Minimax strategy is shown below, at move 53, our agent black’s board situation is not very promising. The current board count for black is 21 and count for white is 37. It's black’s turn to move, black could capture the block (1, 6) to turn 8 white pieces, which is a short-sighted greedy move since this move would give the opening for white player to capture the top-right corner and over turn 6 black pieces at once.

Instead, our Minimax agent chooses to place a piece at (5, 0), which turns 3 white pieces, of which three of the captured pieces are stabilized. Although the current gain is not as much as the greedy move, this move strengthened black’s lower-left corner, at the end of the game, black made a come-back, with a result of 45 to 19.

![](https://github.com/COMP90054-2022S2/comp90054-a3-reversi-ai/blob/Wiki/wiki-template/images/turnover.gif)

[Back to top](#table-of-contents)

### Solved Challenges

#### 1. Improvement of evaluation method

In the original version, we used the evaluation provided by the system, that is, the calScore function provided by ReversiState. This method can show the number of our own pieces on the current board, but it cannot show the number of the opponent agents' pieces. Therefore, as shown above, we calculate the influence of each opponents' chess pieces in evaluation.

#### 2. Improvement of search speed

When using minimax model to build evaluation tree at the beginning, we need to analyze every possible game state. Because the board area is large, minimax does not have the ability to search all potential actions. Moreover, our game system has a timeout penalty for agent response. Therefore, in order to improve the efficiency of minimax, we have made two improvements:

1. As shown above, in the minimax function, we added the judgment of the current depth `"if depth==self.depth"`. Here self.depth is set to 3, which means that minimax will not continue to expand when it reaches the third level. This method can effectively prevent the agent from being punished due to response timeout.
2. As shown above, in the minimax function, we used the Alpha-Beta-Pruning method. When Minimax searches for the corresponding extreme value in each layer, if it is determined that no updated extreme value will appear in the node that has not been traversed subsequently, the node and its subtree will not be traversed. This method effectively reduces the width of the search tree.

![](https://miro.medium.com/max/960/0*FNVaB19gXIyfKtXI.gif)

[Back to top](#table-of-contents)


### Trade-offs  

#### *Advantages*  

- Minimax utilize one aspect of game theory that during the searching process, it always predicts the next move based on opponent's best move. 
- Comparing to other tree searching algorithm, minimax use a-b pruning to eliminate the edges and its following nodes that will not be selected. In this way the computational power needed is drecreased and the total searching efficiency is increased.

#### *Disadvantages*

- Although we could use a-b pruning to decrease the nodes traveled, a complete travel (until end of a game) is still too complicated. Hence, a certain depth is required to be set to return the best action within given time. Based on our tests, a max depth of 4 is achievable. However, due to the relatively shallow depth, the performance is limited.

- The key aspect of a good performing minimax algorithm agent is the evaluation function. However, when competing with other agents, it is difficult to predict precisely the opponent is going to make since the opponent might be using a total different algorithm. Hence, when we traversal nodes in our minimax tree, the prediction can varies from the actual opponent's action.

[Back to top](#table-of-contents)

### Future improvements

- The evaluation strategy we tested is simply counting the blocks taken at a certain board situation. Although it is the solo-measurement of the end game results, it is way too greedy and can even be misleading during the early stage of a game. A more sophisticated evaluation function can be implemented such as involving mobility and stability. Moreover, we could dynamically change the evaluation function based on total moves made (to decide stage of a game). To combine the minimax algorithm with domain knowledge, we could have a much better performing minimax AI agent.

- Minimax has certain advantages that can boost the agent performance when all criteria are met. However its disadvantages means that a lot hard-coded domain knowledge need to be used to assist itself. To avoid this, we could use neural network models to train the agent to find a good evaluation strategy by its own. We could also combine minimax with other algorithms to comprehensively make decisions.